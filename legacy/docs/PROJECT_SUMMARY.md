# Autonomous Art Outreach Scraper - Project Summary

## ğŸ¯ Project Overview

A production-grade Python web application that autonomously scrapes art-related websites, extracts contact information, generates personalized outreach emails using AI, and sends them automatically - running 24/7.

## âœ… Completed Phases

### Phase 1: Foundation
- âœ… Complete folder structure
- âœ… FastAPI backend setup
- âœ… SQLAlchemy database models
- âœ… Configuration management
- âœ… All module placeholders

### Phase 2: Scraping Engine
- âœ… Website scraping with JS fallback (Playwright)
- âœ… Social media scraping (Instagram, TikTok, Behance, Pinterest)
- âœ… Art detection with 7 categories
- âœ… Quality filtering (domain authority, traffic, SSL, DNS)
- âœ… Rate limiting and error handling

### Phase 3: Contact Extraction
- âœ… Email extraction (regex + BeautifulSoup)
- âœ… Phone number extraction
- âœ… Social link extraction (10+ platforms)
- âœ… Contact form detection
- âœ… Contact page crawler (/contact, /about, etc.)
- âœ… Database storage

### Phase 4: LLM Integration
- âœ… Gemini API integration
- âœ… OpenAI API integration
- âœ… Business context-aware email generation
- âœ… Personalized outreach emails

### Phase 5: Email Sender
- âœ… Gmail API integration
- âœ… SMTP support
- âœ… HTML email formatting
- âœ… Retry logic (3 attempts with exponential backoff)
- âœ… Database logging

### Phase 6: Automation Pipeline
- âœ… 24/7 automation with APScheduler
- âœ… 5 scheduled jobs:
  1. Fetch new websites (weekly)
  2. Scrape pending websites (every 6h)
  3. Extract contacts (every 4h)
  4. Generate AI emails (every 2h)
  5. Send emails (hourly)
- âœ… Database job logging

### Phase 7: Dashboard API
- âœ… `/leads` - Get leads with pagination
- âœ… `/emails/sent` - Get sent emails
- âœ… `/emails/pending` - Get pending emails
- âœ… `/scrape-url` - Manual URL scraping
- âœ… `/stats` - Comprehensive statistics
- âœ… `/jobs/status` - Job status monitoring
- âœ… `/jobs/latest` - Latest job executions

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ api/                    # REST API endpoints
â”‚   â”œâ”€â”€ routes.py          # Main API routes
â”‚   â””â”€â”€ dashboard_routes.py # Dashboard endpoints
â”œâ”€â”€ ai/                     # LLM integrations
â”‚   â”œâ”€â”€ gemini_client.py
â”‚   â”œâ”€â”€ openai_client.py
â”‚   â””â”€â”€ email_generator.py
â”œâ”€â”€ db/                     # Database
â”‚   â”œâ”€â”€ database.py        # SQLAlchemy setup
â”‚   â””â”€â”€ models.py           # Data models
â”œâ”€â”€ emailer/                # Email sending
â”‚   â”œâ”€â”€ gmail_client.py
â”‚   â”œâ”€â”€ smtp_client.py
â”‚   â”œâ”€â”€ html_formatter.py
â”‚   â”œâ”€â”€ outreach_email_sender.py
â”‚   â””â”€â”€ email_sender.py
â”œâ”€â”€ extractor/              # Contact extraction
â”‚   â”œâ”€â”€ email_extractor.py
â”‚   â”œâ”€â”€ phone_extractor.py
â”‚   â”œâ”€â”€ social_extractor.py
â”‚   â”œâ”€â”€ contact_form_extractor.py
â”‚   â”œâ”€â”€ contact_page_crawler.py
â”‚   â””â”€â”€ contact_extraction_service.py
â”œâ”€â”€ jobs/                   # Background jobs
â”‚   â”œâ”€â”€ scheduler.py        # APScheduler setup
â”‚   â”œâ”€â”€ automation_jobs.py # 5 automation jobs
â”‚   â””â”€â”€ website_discovery.py
â”œâ”€â”€ scraper/                # Web scraping
â”‚   â”œâ”€â”€ base_scraper.py
â”‚   â”œâ”€â”€ website_scraper.py
â”‚   â”œâ”€â”€ social_scraper.py
â”‚   â”œâ”€â”€ scraper_service.py
â”‚   â”œâ”€â”€ domain_analyzer.py
â”‚   â”œâ”€â”€ art_detector.py
â”‚   â””â”€â”€ rate_limiter.py
â”œâ”€â”€ utils/                  # Utilities
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ logging_config.py
â”œâ”€â”€ main.py                 # FastAPI app
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ seed_websites.txt
â””â”€â”€ README.md
```

## ğŸ—„ï¸ Database Models

1. **ScrapedWebsite** - Websites with quality metrics
2. **Contact** - Emails, phones, social links
3. **ContactForm** - Detected contact forms
4. **OutreachEmail** - Generated and sent emails
5. **ScrapingJob** - Background job logs

## ğŸš€ Quick Start

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   playwright install chromium
   ```

2. **Configure environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

3. **Run the application:**
   ```bash
   python main.py
   ```

4. **Access API:**
   - API: http://localhost:8000
   - Docs: http://localhost:8000/docs
   - Health: http://localhost:8000/health

## ğŸ“Š Dashboard Endpoints

All endpoints at `/api/v1/`:

- `GET /leads` - Leads with pagination
- `GET /emails/sent` - Sent emails
- `GET /emails/pending` - Pending emails
- `POST /scrape-url` - Scrape URL
- `GET /stats` - Statistics
- `GET /jobs/status` - Job status
- `GET /jobs/latest` - Latest jobs

## ğŸ”„ Automation Pipeline

The system runs 24/7 with these scheduled jobs:

1. **Weekly Discovery** (Monday 3 AM)
   - Searches for new art websites
   - Pulls from seed list

2. **Scraping** (Every 6 hours)
   - Processes pending websites
   - Applies quality filters

3. **Contact Extraction** (Every 4 hours)
   - Extracts emails, phones, social links
   - Crawls contact pages

4. **Email Generation** (Every 2 hours)
   - Generates AI emails for contacts
   - Uses business context

5. **Email Sending** (Hourly)
   - Sends draft emails
   - Retry logic included

## ğŸ¨ Target Categories

1. Interior Decor sites
2. Art Gallery sites
3. Home tech sites
4. Mom blogs sites
5. Tech sites for NFTs
6. Editorial media houses
7. Holiday/family oriented sites

## ğŸ”§ Configuration

Key settings in `.env`:

```env
# AI
GEMINI_API_KEY=your_key
OPENAI_API_KEY=your_key

# Email
GMAIL_CLIENT_ID=your_id
GMAIL_CLIENT_SECRET=your_secret
GMAIL_REFRESH_TOKEN=your_token

# Quality Filtering
MIN_QUALITY_SCORE=50
MIN_DOMAIN_AUTHORITY=30
REQUIRE_SSL=True
```

## ğŸ“ˆ Features

- âœ… Automatic website discovery
- âœ… Quality-based filtering
- âœ… Multi-category detection
- âœ… Comprehensive contact extraction
- âœ… AI-powered email generation
- âœ… Automated email sending
- âœ… 24/7 operation
- âœ… Full database logging
- âœ… Dashboard API ready
- âœ… TypeScript/Next.js compatible

## ğŸ¯ Next Steps

1. **Frontend Development:**
   - Build Next.js dashboard
   - Connect to API endpoints
   - Display stats and leads

2. **Enhancements:**
   - Add Google/Bing search APIs
   - Implement email tracking
   - Add analytics dashboard
   - Create reporting features

3. **Deployment:**
   - Docker containerization
   - Production database (PostgreSQL)
   - Environment setup
   - Monitoring and alerts

## ğŸ“ Notes

- All logs stored in database
- Jobs run automatically 24/7
- Quality filtering ensures high-quality leads
- Retry logic handles failures gracefully
- API ready for TypeScript/Next.js frontend

